{"cells":[{"metadata":{"_uuid":"64af9fa3-70b9-4e12-885e-e1fc1d664ef0","_cell_guid":"a12d1971-9343-4ece-b0ec-d99dab8b89cc","trusted":true},"cell_type":"markdown","source":"# Ingest data"},{"metadata":{"_uuid":"b755b9a6-9f7c-428d-8d52-4ee4e22a3078","_cell_guid":"67ba97a1-0a48-4603-aac6-74ca9101f452","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f223cf9b-3830-4027-a37e-7c022845780f","_cell_guid":"23cd711f-a0da-4f1b-b339-3992404f72f1","trusted":true},"cell_type":"code","source":"# data = pd.read_csv(\"./dataset/diabetes.csv\")\ndata = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04fbf148-e84b-4b6c-bac5-281dc385d661","_cell_guid":"3ba37e44-6877-4d1d-b4ad-82df09203744","trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4201079-2662-41c6-a872-bdd4ee7c0b35","_cell_guid":"cc909eba-798c-4060-b8c9-19bbbdd7d07b","trusted":true},"cell_type":"markdown","source":"# Analysis"},{"metadata":{"_uuid":"dbe16c66-a22e-411d-8506-dbb78cc455ae","_cell_guid":"b7e2bda5-7c97-4a1e-838a-25ce92f9bcc7","trusted":true},"cell_type":"markdown","source":"## 1) Data Exploration"},{"metadata":{"_uuid":"462cce67-d01b-4f02-a8fc-4f0176c0c507","_cell_guid":"096ae360-63f6-437b-99b2-d97e47d92ac7","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36b81b3b-91d2-40bc-b609-758385d35f8d","_cell_guid":"9cd636ea-e67a-4013-8250-e9426243b6f7","trusted":true},"cell_type":"markdown","source":"Lets take a look at the data to determine if we have any null values and the datatypes associated to each column.\nIt is important to deal with null values and categorical columns as machine learning algorithms dont do well with sparse dataset and understand only numeric values.\nIn case we find any categorical values we will need to convert them to numeric and also perform one hot encoding in  order to remove bias.\nIn our dataset we have all numeric columns and there are no null values present in the dataset."},{"metadata":{"_uuid":"42703d99-af6d-4799-91af-9a801a69196c","_cell_guid":"9191200a-3e00-41fd-92cc-1712a8f83c14","trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e7f044c-3c0a-4fae-8ad5-2c61b48bc53a","_cell_guid":"0feef7b0-a858-441b-ab2b-950c281c7d2a","trusted":true},"cell_type":"markdown","source":"### Check how many other missing(zero) values"},{"metadata":{"_uuid":"d502dce8-4777-482c-905c-359996600aa9","_cell_guid":"c85ccd50-0a6c-437a-a0f4-50e20be76d5b","trusted":true},"cell_type":"code","source":"print(\"total number of rows : {0}\".format(len(data)))\nprint(\"number of rows missing Glucose: {0}\".format(len(data.loc[data['Glucose'] == 0])))\nprint(\"number of rows missing BloodPressure: {0}\".format(len(data.loc[data['BloodPressure'] == 0])))\nprint(\"number of rows missing SkinThickness: {0}\".format(len(data.loc[data['SkinThickness'] == 0])))\nprint(\"number of rows missing Insulin: {0}\".format(len(data.loc[data['Insulin'] == 0])))\nprint(\"number of rows missing BMI: {0}\".format(len(data.loc[data['BMI'] == 0])))\nprint(\"number of rows missing DiabetesPedigreeFunction: {0}\".format(len(data.loc[data['DiabetesPedigreeFunction'] == 0])))\nprint(\"number of rows missing Age: {0}\".format(len(data.loc[data['Age'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_data = data[data.columns[:-1]]\ntotal = (features_data==0).sum().sort_values(ascending=False)\npercent = (((features_data==0).sum()/(features_data==0).count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c69d17d5-c323-4145-a7da-32a45c1c15d9","_cell_guid":"2988810a-45cf-45f4-ace2-d54dc3953e44","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50415bba-ac73-4ef3-913b-ccb29919b67f","_cell_guid":"9bb487e0-cc13-4a30-a0b5-a6ffc617a750","trusted":true},"cell_type":"markdown","source":"describe() gives out a lot of information.\n* Number of rows and columns in the dataset\n* A number of summary statistics about the dataset such as\n     * Minimum value\n     * Mean value\n     * Maximum value\n     * Standard deviation value\n     * Percentiles\n"},{"metadata":{"_uuid":"6c18bd63-f7ef-401f-addc-10c4d16adcdc","_cell_guid":"5d8ae108-3984-48ca-9301-e88967c71672","trusted":true},"cell_type":"markdown","source":"We can now take a look at some sample records in our dataset."},{"metadata":{"_uuid":"1a690dd1-c8f4-468f-84bb-7fbc5189d018","_cell_guid":"dac604d9-a03c-4b7c-87b4-7b16fbdc3ffe","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f205b70-abb5-4798-8365-aa8e4e5aa7c3","_cell_guid":"d7902c99-e39a-448b-b71f-0b1e1456e135","trusted":true},"cell_type":"markdown","source":"## 2) Visualizing Data"},{"metadata":{"_uuid":"393f7313-8e4f-4081-9538-4aed943d7c5a","_cell_guid":"b87fb0e1-1acc-4402-acc2-a500161f56f9","trusted":true},"cell_type":"code","source":"data.hist(figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"983e1cec-b7a5-478b-b9c7-77716c2b1195","_cell_guid":"fb4d9bb8-d373-4691-93fd-a55f0b95a49b","trusted":true},"cell_type":"markdown","source":"We can detect skewness in our dataset using skew function in scipy.stats"},{"metadata":{"_uuid":"b64fac2d-ea96-4d7e-9dc3-fe882309f2c1","_cell_guid":"12913d98-b984-4d54-a989-01c1fc140884","trusted":true},"cell_type":"code","source":"from scipy.stats import skew\ndata.iloc[:, :-1].apply(lambda x: skew(x.dropna().astype(float)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8c6a493-4b99-46ff-a481-cb74aa382d57","_cell_guid":"7d83b0f5-3f37-4ee1-ad9c-63ad3c0507df","trusted":true},"cell_type":"markdown","source":"As verified using our histograms of individual feature distribution and the scipy stats function insulin, DiabetesPedigreeFunction, Age and Pregnancy are positively skewed"},{"metadata":{"_uuid":"53681fb3-1e87-4277-a9ab-c30898806852","_cell_guid":"4a11e18c-e3c1-4cc3-9846-903e2bcef318","trusted":true},"cell_type":"markdown","source":"Now let us also look at the correlation of the individual features. We can use the corr() function. We can use pair grid to visualize the distribution of features in the dataset corresponding to each other and the outcomes."},{"metadata":{"_uuid":"9f20a2a5-b0f1-4802-98fc-e945aabe6a44","_cell_guid":"314441fe-6efa-4c6b-ae3d-d1edf9a951c4","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\ncorr = data.corr()\ncorr.index = data.columns\nsns.heatmap(corr, annot = True, cbar=True, vmin=-1, vmax=1, square = True)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d8b914-412e-4ed0-a12c-d98bf1df8967","_cell_guid":"f3d1322d-3d33-40ff-842d-99b95f9f90b9","trusted":true},"cell_type":"markdown","source":" We didnt find any strong correlation between a person being diabetic and any independent variable as a result we cannot remove any independent variable from our analysis."},{"metadata":{"_uuid":"098060dc-d5fd-474a-b9cd-fcaa4dd05670","_cell_guid":"288dab1c-ab36-4488-a822-9aa2ac762650","trusted":true},"cell_type":"markdown","source":"# Baseline models"},{"metadata":{},"cell_type":"markdown","source":"### Using Cross Validation\nMany a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n- The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n- Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts. \n- We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm. This is called K-Fold Cross Validation.\n- An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model."},{"metadata":{"_uuid":"47b60f85-fd4f-4852-a1c2-a154c81ded28","_cell_guid":"38783446-c369-47eb-9d7e-2378728ebfd3","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nfeatures = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\noutcomes = ['Outcome']\n\nX = data[features]\ny = data[outcomes]\n\nskf = StratifiedKFold(n_splits=10, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbb752e1-bab4-4b14-b16f-154c82c7adb1","_cell_guid":"1481402a-c464-426b-b11e-806dfb647a02","trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nmodel_names = [\"Linear SVM\",\"Radial SVM\", \"Logistic Regression\", \"Decision Tree\"]\nmodels = [svm.SVC(kernel=\"linear\"), svm.SVC(kernel=\"rbf\"), LogisticRegression(), DecisionTreeClassifier(criterion=\"entropy\")]\n\nmean_accuracy = []\nmean_f1 = []\nglobal_accuracy = []\nglobal_f1 = []\n\ndef evaluate_models(X=X, y=y):\n   '''\n       This method performs 10 fold cross validation on list of models and returns a dataframe with mean f1 and mean accuracy scores for each model\n   '''\n   for name, model in zip(model_names, models):\n        accuracy=[] \n        f1 = []\n        \n        for train_index, test_index in skf.split(X, y):\n            \n            X_train = X.loc[train_index] \n            y_train = y.loc[train_index]\n            X_test = X.loc[test_index]\n            y_test = y.loc[test_index]\n            \n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            accuracy.append(metrics.accuracy_score(y_test, y_pred))\n            f1.append(metrics.f1_score(y_test, y_pred))\n            \n        global_accuracy.append(accuracy)\n        global_f1.append(f1)\n        mean_accuracy.append(np.mean(np.array(accuracy)))\n        mean_f1.append(np.mean(np.array(f1)))\n        \n   model_perf_df = pd.DataFrame(np.array([mean_accuracy, mean_f1]).T,index=model_names)   \n   model_perf_df.columns = ['Mean Accuracy', 'Mean F1']\n   return model_perf_df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_box_plot():\n    '''Generate box plots to visualize the Mean Accuracy and Mean F1 scores calculated across multiple models'''\n    box=pd.DataFrame(data=global_accuracy,index=[model_names])\n    plt.figure(figsize=(20, 20))\n    sns.boxplot(data=box.T).set_title('Mean Accuracy')\n    \n    box=pd.DataFrame( data=global_f1,index=[model_names])\n    plt.figure(figsize=(20, 20))\n    sns.boxplot(data=box.T).set_title('Mean F1 score')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c05a28b-7b58-4798-98cd-1a82dc7eba4f","_cell_guid":"564e0b90-3e44-4803-986f-2133817bbd61","trusted":true},"cell_type":"code","source":"## define utility methods\ndef convert_to_dataframe(ndar, cols):\n    '''Given a set of records in nupy array and a list of column names return a Pandas dataframe'''\n    pdf = pd.DataFrame.from_records(ndar)\n    pdf.columns = cols\n    return pdf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f727f4-f6cd-49e8-a168-3a591281adf1","_cell_guid":"b61a5768-ef5f-4e5a-a522-47321f1e6c79","trusted":true},"cell_type":"code","source":"evaluate_models().sort_values(ascending=False, by = 'Mean F1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_box_plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f948674b-2ed4-48a5-a2d9-e05d57f7124c","_cell_guid":"39f6cae4-cd2e-4a8a-a141-7118af9f5a10","trusted":true},"cell_type":"markdown","source":"# Preprocessing Data"},{"metadata":{"_uuid":"56a0c5f4-b863-4990-990c-9e378d8d964a","_cell_guid":"dc51173f-7190-46d7-923a-e63a77f0f0f3","trusted":true},"cell_type":"markdown","source":"## 1) Impute missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_no_pregnancy_no_skinthickness =  features.copy()\nfeatures_no_pregnancy_no_skinthickness.remove('Pregnancies')\nfeatures_no_pregnancy_no_skinthickness.remove('SkinThickness')\nfeatures_no_pregnancy = features_no_pregnancy_no_skinthickness.copy()\nfeatures_no_pregnancy.append('SkinThickness')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2193bbde-6692-45f6-b16e-5456382a8a04","_cell_guid":"d8e2271f-b232-478b-97a9-e6ddf82186e8","trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer as  Imputer\n\nfill_values = Imputer(missing_values=0, strategy=\"mean\")\nX[features_no_pregnancy_no_skinthickness] = fill_values.fit_transform(X[features_no_pregnancy_no_skinthickness])\n\nfill_values = Imputer(missing_values=0, strategy=\"median\")\nX[features_no_pregnancy] = fill_values.fit_transform(X[features_no_pregnancy])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfccf08f-aabc-4a4c-810b-1b1b56b1dd33","_cell_guid":"fc6378fd-1f2c-4127-b64a-40491264337a","trusted":true},"cell_type":"markdown","source":"## 2) Standardisation\nThere can be a lot of deviation in the given dataset. An example in the dataset can be the BMI where it has 248 unique values. This high variance has to be standardised. Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1."},{"metadata":{"_uuid":"bd549ebd-9d07-45f3-93a2-4d75c93e9773","_cell_guid":"f2cfc8ab-575a-473b-b7cd-c27f9b4ec635","trusted":true},"cell_type":"code","source":"# Feature scaling with StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nscale_features_std = StandardScaler()\nX = scale_features_std.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de4a0938-213a-4549-a2b4-8c0d8e97c5ed","_cell_guid":"02c0864a-7da1-4730-a507-b23328701e8b","trusted":true},"cell_type":"markdown","source":"## 3) Stratification:\nWhen we split the dataset into train and test datasets, the split is completely random. Thus the instances of each class label or outcome in the train or test datasets is random. Thus we may have many instances of class 1 in training data and less instances of class 2 in the training data. So during classification, we may have accurate predictions for class1 but not for class2. Thus we stratify the data, so that we have proportionate data for all the classes in both the training and testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = convert_to_dataframe(X, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.hist(figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.skew()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ace6fd6-86f2-4ae4-a533-f14d7788b8c3","_cell_guid":"af0fc6d9-1d8d-4a48-ab48-89f3cdb81abf","trusted":true},"cell_type":"markdown","source":"# Model Training"},{"metadata":{},"cell_type":"markdown","source":"\n### Model performance after preprocessing data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_accuracy = []\nmean_f1 = []\nglobal_accuracy = []\nglobal_f1 = []\nevaluate_models().sort_values(ascending=False, by = 'Mean F1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"054685c5-91a4-4563-9e2e-51f704ae1cf1","_cell_guid":"e7fc9a4a-5ee7-419e-87f5-ca8cb01ee0aa","trusted":true},"cell_type":"markdown","source":"## Identifying most important features with the Random Forest Classifier"},{"metadata":{"_uuid":"ed6dcf40-8f18-4acb-8e97-ca1e03308622","_cell_guid":"14817b67-e2cf-42dc-ae5a-8953aa093871","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier \nmodel= RandomForestClassifier(n_estimators=250,random_state=10)\nmodel.fit(X,y)\npd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2c058ae-75e7-459d-96f0-b9afb13274b7","_cell_guid":"054846c9-a8f5-439d-b2a3-8304fd59defa","trusted":true},"cell_type":"markdown","source":"The important features are: Glucose, BMI, Age, DiabetesPedigreeFunction. Taking only the important features and rerunning the model training"},{"metadata":{"_uuid":"45f07d8f-3bbd-4005-a4ae-18694f03b03c","_cell_guid":"afc3e218-ccfa-4e0a-8145-15179cd5c81c","trusted":true},"cell_type":"markdown","source":"# Baseline model summary with top features"},{"metadata":{"_uuid":"528861d7-d8eb-4d6f-abdb-6e117eb7f8e5","_cell_guid":"ba08c673-589a-4f04-b345-fa6ef23bcdab","trusted":true},"cell_type":"code","source":"most_important_features = [\"Glucose\", \"BMI\", \"Age\", \"DiabetesPedigreeFunction\" ] \nX_most_important = X[most_important_features]\ny_most_important = y[outcomes]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"940e5bc9-1f5c-4692-8c51-d52823d1a412","_cell_guid":"3fd52e69-f932-44e0-b8eb-9a76f076b0d2","trusted":true},"cell_type":"code","source":"mean_accuracy = []\nmean_f1 = []\nglobal_accuracy = []\nglobal_f1 = []\nevaluate_models(X=X_most_important, y=y_most_important)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_box_plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78b606e1-8a8b-47d3-88ad-60b59b2f3acb","_cell_guid":"59b3f83a-8236-4ec9-b9be-ea864cb85edc","trusted":true},"cell_type":"markdown","source":"Taking into consideration the top 4 features made the  performance improvement and mean accuracy improved by certain points."},{"metadata":{"_uuid":"03706010-3df3-47a8-9505-b1b6a0a63981","_cell_guid":"0b5e5c79-dd2d-4274-917e-3ca389c049d4","trusted":true},"cell_type":"markdown","source":"# Advanced Modelling techniques to improve model performance"},{"metadata":{"_uuid":"5a316c46-1b45-4a7a-8b06-65d6c8916a96","_cell_guid":"32a263dd-912a-4724-9dd9-0888f5bcc0de","trusted":true},"cell_type":"markdown","source":"### Ensembling\nEnsemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. The models used to create such ensemble models are called ‘base models’.\n\nWe will do ensembling with the Voting Ensemble. Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nWe will be using weighted Voting Classifier. We will assign to the classifiers according to their accuracies. So the classifier with single accuracy will be assigned the highest weight and so on.\n\nIn our case, we will use the Top 3 classifiers i.e Linear SVM, Radial SVM and Logistic Regression classifiers."},{"metadata":{"_uuid":"30921e32-f0a1-485d-a868-63b67bcb484d","_cell_guid":"697bae41-5f80-42e1-83c2-509642658e62","trusted":true},"cell_type":"markdown","source":"#### Logistic regression, Linear SVM, Radial SVM"},{"metadata":{"_uuid":"3bff7b36-8cf0-4952-aeb5-a8a46c2c96d8","_cell_guid":"5c33bbe0-df07-4eb4-83d1-61c74d7d5c69","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier #for Voting Classifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb857f0f-ce2e-4306-82f0-7fb7b8faa99a","_cell_guid":"447a40b5-21a0-4f6a-bf1f-5c32d2e4c57d","trusted":true},"cell_type":"code","source":"radial_svc=svm.SVC(kernel='rbf', probability=True)\nlinear_svc=svm.SVC(kernel='linear', probability=True)\nlr=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abc56046-4da7-4a1e-94a6-8d680fd206ce","_cell_guid":"f709b711-dfb3-406f-83f0-4562ee0c6b36","trusted":true},"cell_type":"code","source":"ensemble_lin_rad_lr=VotingClassifier(estimators=[('Linear_svm', linear_svc), ('Radial_SVM', radial_svc),('Logistic Regression', lr)], voting='soft', weights=[0.2, 0.1, 0.1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ba403db-c90d-474c-a950-5746af4742b7","_cell_guid":"04d07bbf-5c17-4705-be36-5c3b4c3945fa","trusted":true},"cell_type":"code","source":"model_names.append(\"Logistic regression, Linear, radial SVM Ensemble model\")\nmodels.append(ensemble_lin_rad_lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00c8c88d-b8a2-4165-955a-4bcb14a8f24f","_cell_guid":"b7e03eee-606c-4ca1-a47a-49e29d80bcde","trusted":true},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"_uuid":"4a088bee-dd37-42e6-a1de-cc3c9708eb1d","_cell_guid":"8d6015c5-39ac-4085-bd63-539345512218","trusted":true},"cell_type":"code","source":"import sys\n# !{sys.executable} -m pip install xgboost\n!{sys.executable} -m conda install -y -c  anaconda py-xgboost","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ae11d2-85a4-42b3-aa97-5c242799499b","_cell_guid":"cf0ce784-c906-4a1d-bf37-2b03c8327752","trusted":true},"cell_type":"code","source":"## Hyper Parameter Optimization\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc192489-a0f0-43e5-a9a4-5813a4162f74","_cell_guid":"456b033b-6011-4ec9-88aa-52454b26e6be","trusted":true},"cell_type":"code","source":"## Hyperparameter optimization using RandomizedSearchCV and getting the best estimator using cross validation\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce2772ab-df69-46ce-8382-65f927dde65a","_cell_guid":"37ffb030-61a6-47b5-83a6-8b14a07d7eb9","trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nclassifier = xgb.XGBClassifier()\nrandom_search = RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='f1',n_jobs=-1,cv=10,verbose=3, random_state=10 )\nrandom_search.fit(X, y)\n#get best estimator\nrandom_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44cdcfcd-0707-4507-b83f-704948a4887c","_cell_guid":"8df3bf81-edfa-45e0-afbc-4ae49f696a6f","trusted":true},"cell_type":"markdown","source":"Using the best estimator to define a classifier"},{"metadata":{"_uuid":"4cf31e48-1d96-4ace-bff3-e88aa0284412","_cell_guid":"f4647db9-b5f4-48cf-b833-cd13b3276652","trusted":true},"cell_type":"code","source":"classifier=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, gamma=0.2,\n              learning_rate=0.05, max_delta_step=0, max_depth=3,\n              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5588c07a-3ab8-48a5-81e3-bf12fbfce59c","_cell_guid":"da5f4a27-b2f5-47a0-ace4-ea89f0071b64","trusted":true},"cell_type":"code","source":"model_names.append(\"Xgboost\")\nmodels.append(classifier) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_accuracy = []\nmean_f1 = []\nglobal_accuracy = []\nglobal_f1 = []\nevaluate_models(X=X_most_important, y=y_most_important).sort_values(ascending=False, by = 'Mean F1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc0daa6-b817-4c4b-970f-05d5920fc697","_cell_guid":"4c2ced25-270f-421f-a270-c65fd8319b52","trusted":true},"cell_type":"code","source":"generate_box_plot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"name":"Predict Diabetes","notebookId":5327248},"nbformat":4,"nbformat_minor":1}